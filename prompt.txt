Role & Objective: Act as a Senior Solution Architect and Python Developer. I am preparing a technical demo for an insurance underwriting and claims interview. I need to build an extensible, end-to-end prototype application in Python that analyzes car accident videos using Vision-Language Models (VLMs).

The Concept: The application allows an insurance underwriter or claims adjuster to upload a dashcam or CCTV video. The system processes the video (simulated for now) and outputs a structured report answering key insurance questions (e.g., "Who hit whom?", "Was the light red?", "Severity of damage").

Constraints & Requirements:

Tech Stack: Python, Streamlit (for the UI).

Architecture: Use the Strategy Design Pattern. Create an abstract base class (interface) for the VLM Analyzer so I can plug in different methods later (e.g., LLaVA, GPT-4o, or Video-LLaMA) without changing the UI code.

No Training: The system is inference-only.

Current State: We are building the UI and the Application Skeleton first. The model inference will be a Dummy/Mock implementation returning hardcoded JSON data for now.

Task: Step-by-Step Implementation Plan & Code

Step 1: Project Structure Define a professional directory structure for the project (e.g., separate folders for app, core, analyzers, utils).

Step 2: The Interface (The "Senior" Part) Write the Python code for an abstract base class named AccidentAnalyzer.

It must have a method analyze_video(video_path: str) -> dict.

The output dict should follow a strict schema relevant to insurance (keys: accident_summary, vehicles_involved, liability_indicator, injury_risk, recommended_action).

Step 3: The Mock Implementation Write a class MockVLMAnalyzer that inherits from AccidentAnalyzer.

Implement analyze_video to return a realistic, comprehensive dummy JSON object representing a "rear-end collision at an intersection."

Include simulated "reasoning" trace (e.g., "Frame 45 shows Vehicle A failing to stop...").

Step 4: The Streamlit UI Write the app.py code.

Sidebar: Allow the user to select the "Model Backend" (e.g., "Mock VLM (Demo)", "LLaVA-Next-Video", "GPT-4o Vision").

Main Area: File uploader for video.

Display:

Show the video player.

Dashboard View: Parse the returned JSON to show "Underwriting Flags" (e.g., Red High Risk banners) and "Claims Facts" (bullet points of what happened).

Use Streamlit's st.expander to show the raw JSON or "System Reasoning" for transparency.

Step 5: Execution Instructions Provide the requirements.txt and the exact terminal command to run the app.